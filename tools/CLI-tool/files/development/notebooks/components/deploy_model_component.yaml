name: Deploy model
description: Deploy the model as an inference service with Kserve.
inputs:
- {name: model_name, type: String}
- {name: storage_uri, type: String}
implementation:
  container:
    image: python:3.9
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kserve==0.11.0' 'kfp==1.8.22' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - |2+

      import kfp
      from kfp.v2 import dsl
      from kfp.v2.dsl import *
      from typing import *

      def deploy_model(model_name: str, storage_uri: str):
          """
          Deploy the model as an inference service with Kserve.
          """
          import logging
          from kubernetes import client
          from kserve import KServeClient
          from kserve import constants
          from kserve import V1beta1InferenceService
          from kserve import V1beta1InferenceServiceSpec
          from kserve import V1beta1PredictorSpec
          from kserve import V1beta1SKLearnSpec
          from kubernetes.client import V1ResourceRequirements

          logging.basicConfig(level=logging.INFO)
          logger = logging.getLogger(__name__)

          model_uri = f"{storage_uri}/{model_name}"
          logger.info(f"MODEL URI: {model_uri}")

          namespace = 'kserve-inference'
          kserve_version='v1beta1'
          api_version = constants.KSERVE_GROUP + '/' + kserve_version

          isvc = V1beta1InferenceService(
              api_version = api_version,
              kind = constants.KSERVE_KIND,
              metadata = client.V1ObjectMeta(
                  name = model_name,
                  namespace = namespace,
                  annotations = {'sidecar.istio.io/inject':'false'}
              ),
              spec = V1beta1InferenceServiceSpec(
                  predictor=V1beta1PredictorSpec(
                      service_account_name="kserve-sa",
                      min_replicas=1,
                      max_replicas = 1,
                      sklearn=V1beta1SKLearnSpec(
                          storage_uri=model_uri,
                          resources=V1ResourceRequirements(
                              requests={"cpu": "100m", "memory": "512Mi"},
                              limits={"cpu": "300m", "memory": "512Mi"}
                          )
                      ),
                  )
              )
          )
          KServe = KServeClient()
          KServe.create(isvc)

    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - deploy_model
